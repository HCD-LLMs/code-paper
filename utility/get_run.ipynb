{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6791a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from mlflow.tracking import MlflowClient\n",
    "import re, pandas as pd, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635ac131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configure name experiment\n",
    "EXPERIMENT_NAME = \"Prompt_Comparison-rhyno-cyt-img\"\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "client = MlflowClient()\n",
    "exp = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "if exp is None:\n",
    "    raise ValueError(f\"Esperimento '{EXPERIMENT_NAME}' non trovato.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b26db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Retrievere all runs\n",
    "runs_df = mlflow.search_runs(exp.experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46f8fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for _, run_info in runs_df.iterrows():\n",
    "    run_id = run_info.run_id\n",
    "    run = client.get_run(run_id)\n",
    "    params = run.data.params\n",
    "    tags = run.data.tags\n",
    "    prompt_type = tags.get(\"prompt_type\")\n",
    "    temperature = float(params.get(\"temperature\", 0))\n",
    "\n",
    "    # Dowload artifacts\n",
    "    local_root = client.download_artifacts(run_id, path=\"\")  \n",
    "    \n",
    "    # Parse output files\n",
    "    for root, _, files in os.walk(local_root):\n",
    "        for fname in files:\n",
    "            if fname.startswith(\"output_\") and fname.endswith(\".txt\"):\n",
    "                gen = int(fname.split(\"_\")[-1].split(\".\")[0])\n",
    "                with open(os.path.join(root, fname), encoding=\"utf-8\") as f:\n",
    "                    text = f.read()\n",
    "                m = re.search(r\"\\*\\*Tasks\\*\\*([\\s\\S]*)\", text)\n",
    "                if not m:\n",
    "                    continue\n",
    "                tasks_block = m.group(1)\n",
    "                tasks = re.findall(r\"- (.+)\", tasks_block)\n",
    "                for t in tasks:\n",
    "                    rows.append({\n",
    "                        \"prompt_type\": prompt_type,\n",
    "                        \"temperature\": temperature,\n",
    "                        \"generation\": gen,\n",
    "                        \"task\": t.strip()\n",
    "                    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f12e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d24cf09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "prompt_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "temperature",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "generation",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "task",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "09db345e-6ce8-4a12-96f8-2e5e188ea099",
       "rows": [
        [
         "0",
         "few_shot",
         "1.0",
         "2",
         "Check the classifications of cells in the current batch and validate their correctness, correcting any misclassifications."
        ],
        [
         "1",
         "few_shot",
         "1.0",
         "2",
         "Examine the reference range for a specific cell type and determine if it's within the expected limits."
        ],
        [
         "2",
         "few_shot",
         "1.0",
         "2",
         "Review the classified images of a particular cell type, assessing the confidence level and verifying if the displayed image corresponds to the selected cell type."
        ],
        [
         "3",
         "few_shot",
         "1.0",
         "2",
         "Mark a batch of cells as \"Correct\" or \"Incorrect\" with explanations for the errors, using the \"Correct\" option to adjust the model's accuracy."
        ],
        [
         "4",
         "few_shot",
         "1.0",
         "2",
         "Browse the dashboard to understand the overall performance of the system in identifying cell types, looking for patterns or areas for model improvement."
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_type</th>\n",
       "      <th>temperature</th>\n",
       "      <th>generation</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>few_shot</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Check the classifications of cells in the curr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>few_shot</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Examine the reference range for a specific cel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>few_shot</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Review the classified images of a particular c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>few_shot</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Mark a batch of cells as \"Correct\" or \"Incorre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>few_shot</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Browse the dashboard to understand the overall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt_type  temperature  generation  \\\n",
       "0    few_shot          1.0           2   \n",
       "1    few_shot          1.0           2   \n",
       "2    few_shot          1.0           2   \n",
       "3    few_shot          1.0           2   \n",
       "4    few_shot          1.0           2   \n",
       "\n",
       "                                                task  \n",
       "0  Check the classifications of cells in the curr...  \n",
       "1  Examine the reference range for a specific cel...  \n",
       "2  Review the classified images of a particular c...  \n",
       "3  Mark a batch of cells as \"Correct\" or \"Incorre...  \n",
       "4  Browse the dashboard to understand the overall...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9410f6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel generato: tasks_comparison_rhyno.xlsx\n"
     ]
    }
   ],
   "source": [
    "# 6. Export to Excel with formatting\n",
    "excel_path = \"xlsx/tasks_comparison_rhyno.xlsx\"\n",
    "with pd.ExcelWriter(excel_path, engine=\"xlsxwriter\") as writer:\n",
    "    df.to_excel(writer, sheet_name=\"Tasks\", index=False)\n",
    "\n",
    "    workbook  = writer.book\n",
    "    worksheet = writer.sheets[\"Tasks\"]\n",
    "\n",
    "   \n",
    "    header_format = workbook.add_format({\n",
    "        \"bold\": True,\n",
    "        \"text_wrap\": True,\n",
    "        \"align\": \"center\",\n",
    "        \"valign\": \"vcenter\"\n",
    "    })\n",
    "\n",
    "    # Applied header format\n",
    "    for col_num, col_name in enumerate(df.columns):\n",
    "        worksheet.write(0, col_num, col_name, header_format)\n",
    "\n",
    "    \n",
    "    for idx, col in enumerate(df.columns):\n",
    "        \n",
    "        max_len = max(\n",
    "            df[col].astype(str).map(len).max(),\n",
    "            len(col)\n",
    "        ) + 2\n",
    "        worksheet.set_column(idx, idx, max_len)\n",
    "\n",
    "print(f\"Excel generated: {excel_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
